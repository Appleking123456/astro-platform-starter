{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNaKtEwN0SUPxesj6XBqQES",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Appleking123456/astro-platform-starter/blob/main/Successful_rdt_system_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import umap.umap_ as umap\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import warnings\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Suppress specific UserWarnings from UMAP and KMeans\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# ===== CONFIGURATION =====\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Explicitly set default dtype for float operations to float32\n",
        "torch.set_default_dtype(torch.float32) # ADDED THIS LINE\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create directory for saving results\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_dir = f\"rdt_results_{timestamp}\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# ===== ENHANCED RCS AGENT =====\n",
        "class RCSAgent(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, d_model=128, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Enhanced input processing with residual connections\n",
        "        self.input_embedding = nn.Sequential(\n",
        "            nn.Linear(state_dim, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        ).to(device)\n",
        "\n",
        "        # Multi-scale feature extraction\n",
        "        self.local_features = nn.Conv1d(1, d_model//4, kernel_size=3, padding=1).to(device)\n",
        "        self.global_features = nn.Conv1d(1, d_model//4, kernel_size=7, padding=3).to(device)\n",
        "\n",
        "        # Convolutional feature projection layer - Initialized after state_dim is known\n",
        "        conv_feature_dim = self.state_dim * (self.d_model // 4) * 2\n",
        "        self.conv_proj = nn.Linear(conv_feature_dim, self.d_model).to(device)\n",
        "\n",
        "        # Enhanced transformer with more sophisticated architecture\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True,\n",
        "            activation='gelu',\n",
        "            device=device\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4).to(device)\n",
        "\n",
        "        # Attention mechanism for memory\n",
        "        self.memory_attention = nn.MultiheadAttention(\n",
        "            embed_dim=d_model,\n",
        "            num_heads=4,\n",
        "            batch_first=True,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Enhanced actor network with residual connections\n",
        "        self.actor_layers = nn.ModuleList([\n",
        "            nn.Linear(d_model, 128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.Linear(64, action_dim)\n",
        "        ]).to(device)\n",
        "\n",
        "        self.actor_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(128),\n",
        "            nn.LayerNorm(64)\n",
        "        ]).to(device)\n",
        "\n",
        "        # Enhanced critic with uncertainty estimation\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(d_model, 128),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(32, 2)  # [value, uncertainty]\n",
        "        ).to(device)\n",
        "\n",
        "        # Expanded memory with episodic and semantic components\n",
        "        self.register_buffer('episodic_memory', torch.zeros(20, d_model, device=device)) # Stores recent states\n",
        "        self.register_buffer('semantic_memory', torch.zeros(10, d_model, device=device)) # Stores significant states\n",
        "        self.register_buffer('memory_ptr', torch.tensor(0, device=device, dtype=torch.long)) # Pointer for episodic memory\n",
        "        self.register_buffer('semantic_ptr', torch.tensor(0, device=device, dtype=torch.long)) # Pointer for semantic memory\n",
        "\n",
        "        # Meta-learning parameters\n",
        "        self.adaptation_rate = nn.Parameter(torch.tensor(0.01, device=device))\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def update_memory(self, state_embedding, is_significant=False):\n",
        "        # Use .data to update registered buffers without involving autograd\n",
        "        self.episodic_memory.data[self.memory_ptr] = state_embedding.detach().data\n",
        "        self.memory_ptr.data = (self.memory_ptr.data + 1) % self.episodic_memory.size(0)\n",
        "\n",
        "        if is_significant:\n",
        "            self.semantic_memory.data[self.semantic_ptr] = state_embedding.detach().data\n",
        "            self.semantic_ptr.data = (self.semantic_ptr.data + 1) % self.semantic_memory.size(0)\n",
        "\n",
        "    def forward(self, state_sequence):\n",
        "        batch_size = state_sequence.size(0)\n",
        "        seq_len = state_sequence.size(1)\n",
        "\n",
        "        # Multi-scale feature extraction\n",
        "        # Unsqueeze to add a channel dimension for Conv1d, expects (N, C, L)\n",
        "        # Here L is state_dim, C is 1.\n",
        "        last_state = state_sequence[:, -1, :].unsqueeze(1) # (batch_size, 1, state_dim)\n",
        "\n",
        "        # Process through conv layers\n",
        "        local_feat = self.local_features(last_state).transpose(1, 2) # (batch_size, state_dim, d_model//4)\n",
        "        global_feat = self.global_features(last_state).transpose(1, 2) # (batch_size, state_dim, d_model//4)\n",
        "\n",
        "        # The input embedding transforms each state in the sequence\n",
        "        x_seq_embedding = self.input_embedding(state_sequence.float()) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Positional encoding for the transformer sequence\n",
        "        pos_encoding = torch.zeros(seq_len, self.d_model, device=device, dtype=torch.float32) # Explicit dtype\n",
        "        position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * -(np.log(10000.0) / self.d_model))\n",
        "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        x = x_seq_embedding + pos_encoding.unsqueeze(0) # Add positional encoding\n",
        "\n",
        "        # Transformer processing\n",
        "        memory = self.transformer(x)\n",
        "        state_embedding = memory[:, -1, :] # Take the embedding of the last element\n",
        "\n",
        "        # Combine with multi-scale features\n",
        "        combined_conv_features = torch.cat([local_feat.flatten(start_dim=1), global_feat.flatten(start_dim=1)], dim=1)\n",
        "        projected_conv = self.conv_proj(combined_conv_features) # Project to d_model\n",
        "        state_embedding = state_embedding + projected_conv # Add to state embedding\n",
        "\n",
        "        # Memory attention mechanism\n",
        "        # Ensure episodic_memory is not empty\n",
        "        if self.episodic_memory.norm() > 0:\n",
        "            memory_for_attention = self.episodic_memory.detach() # Detach to prevent gradient flow through memory\n",
        "            attended_memory, _ = self.memory_attention(\n",
        "                state_embedding.unsqueeze(1),\n",
        "                memory_for_attention.unsqueeze(0).expand(batch_size, -1, -1),\n",
        "                memory_for_attention.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "            )\n",
        "            state_embedding = state_embedding + 0.1 * attended_memory.squeeze(1)\n",
        "\n",
        "        # Enhanced actor with residual connections\n",
        "        actor_x = state_embedding\n",
        "        for i, (layer, norm) in enumerate(zip(self.actor_layers[:-1], self.actor_norms)):\n",
        "            residual = actor_x\n",
        "            actor_x = layer(actor_x)\n",
        "            actor_x = norm(actor_x)\n",
        "            actor_x = F.silu(actor_x)\n",
        "            # Ensure residual connection is only added if dimensions match\n",
        "            if residual.size(-1) == actor_x.size(-1):\n",
        "                actor_x = actor_x + residual\n",
        "            actor_x = F.dropout(actor_x, 0.1, training=self.training)\n",
        "\n",
        "        action_raw = self.actor_layers[-1](actor_x)\n",
        "        action_raw = torch.tanh(action_raw)\n",
        "\n",
        "        # Enhanced value and uncertainty estimation\n",
        "        value_uncertainty = self.critic(state_embedding)\n",
        "        value = value_uncertainty[:, 0:1]\n",
        "        uncertainty = torch.sigmoid(value_uncertainty[:, 1:2])\n",
        "\n",
        "        # Adaptive action scaling\n",
        "        state_complexity = (state_sequence.std(dim=(1,2)).mean() + 1e-8) # Corrected parenthesis here\n",
        "        exploration_factor = 1.0 + uncertainty.squeeze() * 0.5\n",
        "        scaling_factor = torch.tensor([0.12, 0.06], device=device, dtype=torch.float32) * exploration_factor.unsqueeze(-1) # Explicit dtype\n",
        "        action_scaled = action_raw * scaling_factor\n",
        "\n",
        "        # Update memory\n",
        "        is_significant_state = False # This can be made dynamic based on internal criteria\n",
        "        self.update_memory(state_embedding[0] if batch_size == 1 else state_embedding.mean(0), is_significant_state)\n",
        "\n",
        "        return action_scaled, value, state_embedding, uncertainty\n",
        "\n",
        "# ===== ENHANCED DYNAMIC TENSOR =====\n",
        "class DynamicTensor(nn.Module):\n",
        "    def __init__(self, size=(4,4,4)):\n",
        "        super().__init__()\n",
        "        self.initial_size = size\n",
        "        self.history = []\n",
        "        self.phase_history = []\n",
        "\n",
        "        # Use nn.Parameter for tensor_data, entanglement_level, and chaoticity_index\n",
        "        # so they are part of the model's state_dict and can be saved/loaded\n",
        "        self._tensor_data = nn.Parameter(self._initialize_tensor(size))\n",
        "        self._entanglement_level = nn.Parameter(torch.tensor(0.5, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "        self._chaoticity_index = nn.Parameter(torch.tensor(0.1, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "\n",
        "        # Enhanced learnable evolution parameters\n",
        "        self.evolution_rate = nn.Parameter(torch.tensor(0.08, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "        self.syntropic_strength = nn.Parameter(torch.tensor(0.15, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "        self.quantum_coupling = nn.Parameter(torch.tensor(0.03, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "\n",
        "        # Phase transition parameters\n",
        "        self.critical_entanglement = nn.Parameter(torch.tensor(0.618, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "        self.phase_transition_sharpness = nn.Parameter(torch.tensor(5.0, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "\n",
        "    def _initialize_tensor(self, size):\n",
        "        base_tensor = torch.randn(size, device=device, dtype=torch.float32) # Explicit dtype\n",
        "        pattern = torch.zeros(size, device=device, dtype=torch.float32) # Explicit dtype\n",
        "        for i in range(size[0]):\n",
        "            for j in range(size[1]):\n",
        "                x = torch.linspace(0, 2*np.pi, size[2], device=device, dtype=torch.float32) # Explicit dtype\n",
        "                structure = (0.3 * torch.sin(x + i * np.pi/4) +\n",
        "                           0.2 * torch.cos(2*x + j * np.pi/3) +\n",
        "                           0.1 * torch.sin(3*x))\n",
        "                pattern[i, j] = structure\n",
        "        return base_tensor + pattern\n",
        "\n",
        "    def compute_phase_order(self, tensor_data):\n",
        "        \"\"\"Compute an order parameter for phase transitions\"\"\"\n",
        "        if tensor_data.numel() < 2:\n",
        "            return torch.tensor(0.0, device=device, dtype=torch.float32) # Explicit dtype\n",
        "\n",
        "        flat_data = tensor_data.flatten()\n",
        "        correlations = []\n",
        "\n",
        "        for shift in [1, 2]:\n",
        "            if len(flat_data) > shift:\n",
        "                x_shifted = flat_data[:-shift]\n",
        "                y_unshifted = flat_data[shift:]\n",
        "\n",
        "                mean_x = x_shifted.mean()\n",
        "                mean_y = y_unshifted.mean()\n",
        "\n",
        "                cov_xy = ((x_shifted - mean_x) * (y_unshifted - mean_y)).mean()\n",
        "                std_x = x_shifted.std()\n",
        "                std_y = y_unshifted.std()\n",
        "\n",
        "                if std_x > 1e-8 and std_y > 1e-8:\n",
        "                    corr = cov_xy / (std_x * std_y)\n",
        "                else:\n",
        "                    corr = torch.tensor(0.0, device=device, dtype=torch.float32) # Explicit dtype\n",
        "\n",
        "                correlations.append(corr)\n",
        "\n",
        "        return torch.stack(correlations).mean() if correlations else torch.tensor(0.0, device=device, dtype=torch.float32) # Explicit dtype\n",
        "\n",
        "    # Forward pass for the tensor's internal evolution\n",
        "    def forward(self, current_tensor_data, current_entanglement_level, current_chaoticity_index):\n",
        "        # Ensure all inputs are float32\n",
        "        current_tensor_data = current_tensor_data.to(torch.float32)\n",
        "        current_entanglement_level = current_entanglement_level.to(torch.float32)\n",
        "        current_chaoticity_index = current_chaoticity_index.to(torch.float32)\n",
        "\n",
        "        # Enhanced multi-scale evolution\n",
        "        noise = torch.randn_like(current_tensor_data, dtype=torch.float32) * self.evolution_rate # Explicit dtype\n",
        "        evolved_data = current_tensor_data + noise\n",
        "\n",
        "        # Phase transition dynamics\n",
        "        phase_order = self.compute_phase_order(evolved_data)\n",
        "        entanglement_diff = current_entanglement_level - self.critical_entanglement\n",
        "        phase_transition_factor = torch.tanh(\n",
        "            self.phase_transition_sharpness * entanglement_diff\n",
        "        )\n",
        "\n",
        "        # Enhanced syntropic dynamics\n",
        "        std_val = evolved_data.std() + 1e-8\n",
        "        mean_val = evolved_data.mean()\n",
        "\n",
        "        syntropic_target = (\n",
        "            mean_val +\n",
        "            self.syntropic_strength * std_val * torch.randn_like(mean_val, dtype=torch.float32) * # Explicit dtype\n",
        "            (1 + phase_transition_factor * 0.5)\n",
        "        )\n",
        "\n",
        "        # Multi-dimensional quantum-inspired dynamics\n",
        "        quantum_influence = torch.zeros_like(evolved_data, dtype=torch.float32) # Explicit dtype\n",
        "        if evolved_data.numel() > 1:\n",
        "            # Handle different shapes for FFT2\n",
        "            if evolved_data.dim() > 2: # If 3D, take first slice\n",
        "                fft_input = evolved_data[0].to(torch.float32) # Explicit dtype\n",
        "            elif evolved_data.dim() == 2: # If 2D, use directly\n",
        "                fft_input = evolved_data.to(torch.float32) # Explicit dtype\n",
        "            else: # If 1D, reshape to 2D for FFT2 if possible\n",
        "                num_elements = evolved_data.numel()\n",
        "                side_len = int(np.sqrt(num_elements))\n",
        "                if side_len * side_len == num_elements:\n",
        "                    fft_input = evolved_data.reshape(side_len, side_len).to(torch.float32) # Explicit dtype\n",
        "                else: # Cannot reshape to square, fall back to a simpler influence\n",
        "                    fft_input = evolved_data.unsqueeze(0).to(torch.float32) # Add a batch dim, Explicit dtype\n",
        "\n",
        "            # Ensure fft_input is 2D for torch.fft.fft2\n",
        "            if fft_input.dim() == 1:\n",
        "                fft_input = fft_input.unsqueeze(0) # Make it (1, L)\n",
        "\n",
        "            if fft_input.dim() == 2 and fft_input.numel() > 0: # Check if it's actually 2D and not empty\n",
        "                fft_data = torch.fft.fft2(fft_input)\n",
        "                phase_spectrum = torch.angle(fft_data)\n",
        "                quantum_influence_scalar = self.quantum_coupling * torch.cos(phase_spectrum.real).mean()\n",
        "                quantum_influence = quantum_influence_scalar * torch.randn_like(evolved_data, dtype=torch.float32) # Explicit dtype\n",
        "\n",
        "        # Combine all evolution mechanisms\n",
        "        evolved_data = (\n",
        "            (1 - current_chaoticity_index) * evolved_data +\n",
        "            current_chaoticity_index * syntropic_target +\n",
        "            quantum_influence +\n",
        "            0.05 * phase_transition_factor * torch.randn_like(evolved_data, dtype=torch.float32) # Explicit dtype\n",
        "        )\n",
        "\n",
        "        # Enhanced entanglement-driven stabilization\n",
        "        entanglement_effect = torch.sigmoid((current_entanglement_level - 0.5) * 8)\n",
        "        evolved_data = (\n",
        "            (1 - entanglement_effect) * evolved_data +\n",
        "            entanglement_effect * (0.8 * mean_val + 0.2 * evolved_data)\n",
        "        )\n",
        "\n",
        "        # Store extended history (detached from graph)\n",
        "        with torch.no_grad():\n",
        "            if len(self.history) > 150:\n",
        "                self.history.pop(0)\n",
        "            if len(self.phase_history) > 100:\n",
        "                self.phase_history.pop(0)\n",
        "\n",
        "            self.history.append(evolved_data.detach().cpu().numpy().astype(np.float32)) # Explicitly cast to float32 NumPy array\n",
        "            self.phase_history.append(phase_order.detach().cpu().numpy().astype(np.float32)) # Explicitly cast to float32 NumPy array\n",
        "\n",
        "        return evolved_data\n",
        "\n",
        "    # Getter methods for direct access to the Parameter objects\n",
        "    def get_current_tensor_data(self):\n",
        "        return self._tensor_data\n",
        "\n",
        "    def get_entanglement_level(self):\n",
        "        return self._entanglement_level\n",
        "\n",
        "    def get_chaoticity_index(self):\n",
        "        return self._chaoticity_index\n",
        "\n",
        "    # Setter methods for updating Parameter data\n",
        "    def set_current_tensor_data(self, new_data):\n",
        "        self._tensor_data.data = new_data.data.to(torch.float32) # Explicitly cast new data\n",
        "\n",
        "    def set_entanglement_level(self, new_level):\n",
        "        self._entanglement_level.data = torch.clamp(new_level, 0.01, 0.99).data.to(torch.float32) # Explicitly cast new data\n",
        "\n",
        "    def set_chaoticity_index(self, new_index):\n",
        "        self._chaoticity_index.data = torch.clamp(new_index, 0.001, 0.8).data.to(torch.float32) # Explicitly cast new data\n",
        "\n",
        "    def metrics(self):\n",
        "        # Detach here as metrics are for monitoring, not for gradient computation through these values\n",
        "        t_data = self._tensor_data.detach()\n",
        "        ent_level = self._entanglement_level.detach().item()\n",
        "        cha_index = self._chaoticity_index.detach().item()\n",
        "\n",
        "        # Enhanced metrics\n",
        "        t_flat = t_data.flatten()\n",
        "        t_normalized = F.softmax(t_flat, dim=0) + 1e-8\n",
        "        entropy_val = -(t_normalized * torch.log(t_normalized)).sum().item()\n",
        "        variance = t_data.var().item()\n",
        "        complexity = entropy_val * variance\n",
        "\n",
        "        # Fractal dimension estimation\n",
        "        fractal_dim = 1.5\n",
        "        try:\n",
        "            if len(self.history) > 10:\n",
        "                # Ensure history elements are float32 NumPy arrays\n",
        "                recent_history_flat = np.array(self.history[-10:], dtype=np.float32).flatten()\n",
        "                if len(recent_history_flat) > 0 and np.std(recent_history_flat) > 1e-8:\n",
        "                    data_range = np.max(recent_history_flat) - np.min(recent_history_flat) + 1e-8\n",
        "                    num_boxes = 10\n",
        "                    scales = np.linspace(data_range / num_boxes, data_range, num_boxes, dtype=np.float32)\n",
        "                    counts = []\n",
        "                    for scale in scales:\n",
        "                        boxes = np.floor((recent_history_flat - np.min(recent_history_flat)) / scale).astype(int)\n",
        "                        counts.append(len(np.unique(boxes)))\n",
        "                    log_scales = np.log(scales)\n",
        "                    log_counts = np.log(np.array(counts, dtype=np.float32) + 1e-8)\n",
        "                    if len(log_scales) > 1 and np.std(log_counts) > 1e-8:\n",
        "                        slope, _ = np.polyfit(log_scales, log_counts, 1)\n",
        "                        fractal_dim = -slope\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "        # Phase coherence\n",
        "        phase_coherence = 0.0\n",
        "        if len(self.phase_history) > 5:\n",
        "            recent_phases = np.array(self.phase_history[-5:], dtype=np.float32) # Explicit dtype\n",
        "            mean_abs_phase = np.mean(np.abs(recent_phases)) + 1e-8\n",
        "            if mean_abs_phase > 0:\n",
        "                phase_coherence = 1.0 - np.std(recent_phases) / mean_abs_phase\n",
        "            phase_coherence = np.clip(phase_coherence, 0.0, 1.0)\n",
        "\n",
        "        return {\n",
        "            'entropy': entropy_val,\n",
        "            'entanglement_value': ent_level,\n",
        "            'chaoticity_value': cha_index,\n",
        "            'variance': variance,\n",
        "            'complexity': complexity,\n",
        "            'fractal_dimension': fractal_dim,\n",
        "            'phase_coherence': phase_coherence\n",
        "        }\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset parameters by assigning new Parameter objects or new data to existing ones\n",
        "        self._tensor_data.data = self._initialize_tensor(self.initial_size).data\n",
        "        self._entanglement_level.data = torch.tensor(0.5, device=device, dtype=torch.float32).data # Explicit dtype\n",
        "        self._chaoticity_index.data = torch.tensor(0.1, device=device, dtype=torch.float32).data # Explicit dtype\n",
        "        self.history.clear()\n",
        "        self.phase_history.clear()\n",
        "\n",
        "# ===== ENHANCED CONSCIOUSNESS KERNEL =====\n",
        "class ConsciousnessKernel(nn.Module):\n",
        "    def __init__(self, target_spectrum_size=4):\n",
        "        super().__init__()\n",
        "        self.spectrum_size = target_spectrum_size\n",
        "\n",
        "        # Enhanced learnable frequencies\n",
        "        self.delta_freq = nn.Parameter(torch.tensor(0.5, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "        self.theta_freq = nn.Parameter(torch.tensor(0.8, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "        self.alpha_freq = nn.Parameter(torch.tensor(1.2, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "        self.beta_freq = nn.Parameter(torch.tensor(1.8, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "        self.gamma_freq = nn.Parameter(torch.tensor(2.5, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "\n",
        "        # Dynamic weights with attention mechanism\n",
        "        self.attention_weights = nn.Parameter(torch.ones(5, target_spectrum_size, device=device, dtype=torch.float32)) # Explicit dtype\n",
        "\n",
        "        # Neural entanglement processor\n",
        "        self.entanglement_processor = nn.Sequential(\n",
        "            nn.Linear(1, 32),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(64, target_spectrum_size * 5),\n",
        "            nn.Tanh()\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, tensor_data, entanglement_level):\n",
        "        # Ensure tensor_data is float for calculations\n",
        "        tensor_data = tensor_data.to(torch.float32) # Ensure float32\n",
        "        entanglement_level = entanglement_level.to(torch.float32) # Ensure float32\n",
        "\n",
        "        # Handle scalar input\n",
        "        if tensor_data.dim() == 0:\n",
        "            spatial_norms = [tensor_data.unsqueeze(0)]\n",
        "        else:\n",
        "            spatial_norms = []\n",
        "            for dim_idx in range(len(tensor_data.shape)):\n",
        "                dims_to_reduce = [i for i in range(len(tensor_data.shape)) if i != dim_idx]\n",
        "                if dims_to_reduce:\n",
        "                    if tensor_data.numel() > 1:\n",
        "                        norm_val = torch.norm(tensor_data, dim=dims_to_reduce)\n",
        "                        spatial_norms.append(norm_val.mean().unsqueeze(0))\n",
        "                    else:\n",
        "                        spatial_norms.append(torch.norm(tensor_data).unsqueeze(0))\n",
        "                else:\n",
        "                    spatial_norms.append(torch.norm(tensor_data).unsqueeze(0))\n",
        "\n",
        "        if not spatial_norms:\n",
        "            r_total = torch.norm(tensor_data)\n",
        "            spatial_norms.append(r_total.unsqueeze(0))\n",
        "        else:\n",
        "            r_total = torch.norm(tensor_data)\n",
        "\n",
        "        # Pad or truncate spatial_norms to match spectrum_size\n",
        "        if len(spatial_norms) < self.spectrum_size:\n",
        "            spatial_norms_extended = spatial_norms * ((self.spectrum_size // len(spatial_norms)) + 1)\n",
        "            spatial_norms = spatial_norms_extended[:self.spectrum_size]\n",
        "        elif len(spatial_norms) > self.spectrum_size:\n",
        "            spatial_norms = spatial_norms[:self.spectrum_size]\n",
        "\n",
        "        # Process entanglement\n",
        "        ent_processed_raw = self.entanglement_processor(entanglement_level.unsqueeze(0))\n",
        "        ent_processed = ent_processed_raw.reshape(5, self.spectrum_size) * 0.5 + 0.5\n",
        "\n",
        "        # Generate multi-frequency spectrum components\n",
        "        frequencies = [self.delta_freq, self.theta_freq, self.alpha_freq,\n",
        "                      self.beta_freq, self.gamma_freq]\n",
        "\n",
        "        spectrum_components = []\n",
        "\n",
        "        for freq_idx, freq in enumerate(frequencies):\n",
        "            components = []\n",
        "            for i in range(self.spectrum_size):\n",
        "                phase = (i / self.spectrum_size) * 2 * np.pi\n",
        "                current_spatial_component = spatial_norms[i].squeeze()\n",
        "                arg = freq * ent_processed[freq_idx, i] * (current_spatial_component + r_total * 0.1)\n",
        "\n",
        "                # Use bessel functions and trigonometric terms, ensure phase is float32\n",
        "                if freq_idx == 0:\n",
        "                    value = torch.special.bessel_j0(arg) * torch.cos(torch.tensor(phase, device=device, dtype=torch.float32))\n",
        "                elif freq_idx == 1:\n",
        "                    value = torch.special.bessel_j1(arg) * torch.sin(torch.tensor(phase, device=device, dtype=torch.float32))\n",
        "                elif freq_idx == 2:\n",
        "                    value = torch.special.bessel_j0(arg + np.pi/4) * torch.cos(torch.tensor(phase + np.pi/8, device=device, dtype=torch.float32))\n",
        "                elif freq_idx == 3:\n",
        "                    value = torch.special.bessel_j1(arg + np.pi/3) * torch.sin(torch.tensor(phase + np.pi/6, device=device, dtype=torch.float32))\n",
        "                else: # freq_idx == 4 (gamma)\n",
        "                    value = torch.special.bessel_j0(arg + 0.5) * torch.cos(torch.tensor(phase + np.pi/4, device=device, dtype=torch.float32))\n",
        "\n",
        "                components.append(value)\n",
        "\n",
        "            spectrum_components.append(torch.stack(components))\n",
        "\n",
        "        # Attention-weighted combination\n",
        "        combined_spectrum = torch.zeros(self.spectrum_size, device=device, dtype=torch.float32) # Explicit dtype\n",
        "        attn_weights_norm = F.softmax(self.attention_weights, dim=0)\n",
        "\n",
        "        for freq_idx, spectrum in enumerate(spectrum_components):\n",
        "            combined_spectrum += attn_weights_norm[freq_idx] * spectrum\n",
        "\n",
        "        # Normalize the spectrum\n",
        "        combined_spectrum_min = combined_spectrum.min()\n",
        "        combined_spectrum_max = combined_spectrum.max()\n",
        "        if combined_spectrum_max - combined_spectrum_min > 1e-8:\n",
        "            normalized_spectrum = (combined_spectrum - combined_spectrum_min) / (combined_spectrum_max - combined_spectrum_min)\n",
        "        else:\n",
        "            normalized_spectrum = torch.zeros_like(combined_spectrum, dtype=torch.float32) # Avoid division by zero, explicit dtype\n",
        "\n",
        "        return normalized_spectrum\n",
        "\n",
        "# ===== ENHANCED ENVIRONMENT =====\n",
        "class RDTEnvironment:\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "        self.tensor = DynamicTensor().to(device)\n",
        "        self.kernel = ConsciousnessKernel(target_spectrum_size=self.tensor.initial_size[-1]).to(device)\n",
        "\n",
        "        self.time = 0\n",
        "        self.max_time = 20\n",
        "        self.episode_count = 0\n",
        "        self.difficulty = 1.0  # Initialize difficulty BEFORE calling _generate_adaptive_target\n",
        "        self.target = self._generate_adaptive_target(self.tensor.initial_size[-1]).to(device)\n",
        "        self.performance_history = []\n",
        "        self.prev_action = None\n",
        "        self.current_episode_similarities = [] # Track similarities for current episode\n",
        "        self.state_history_buffer = None # Buffer for agent's state sequence\n",
        "\n",
        "    def _generate_adaptive_target(self, size):\n",
        "        x = torch.linspace(0, 6*np.pi, size, device=self.device, dtype=torch.float32) # Explicit dtype\n",
        "        target = (\n",
        "            0.4 * torch.sin(x) +\n",
        "            0.25 * torch.sin(2*x + np.pi/3) +\n",
        "            0.15 * torch.sin(3*x + np.pi/2) +\n",
        "            0.1 * torch.sin(5*x) +\n",
        "            0.05 * torch.randn(size, device=self.device, dtype=torch.float32) # Explicit dtype\n",
        "        )\n",
        "        return torch.sigmoid(target * self.difficulty)\n",
        "\n",
        "    def update_curriculum(self):\n",
        "        # Only update curriculum after a full episode has been completed and tracked\n",
        "        if self.episode_count > 0 and self.episode_count % 40 == 0:\n",
        "            # Ensure performance_history has enough data\n",
        "            recent_performance = np.mean(self.performance_history[-40:]) if len(self.performance_history) >= 40 else 0\n",
        "            if recent_performance > 0.7:\n",
        "                self.difficulty = min(2.5, self.difficulty * 1.15)\n",
        "            elif recent_performance < 0.3:\n",
        "                self.difficulty = max(0.5, self.difficulty * 0.95)\n",
        "            # Regenerate target with new difficulty\n",
        "            self.target = self._generate_adaptive_target(self.tensor.initial_size[-1])\n",
        "            self.performance_history.clear() # Clear for next cycle\n",
        "\n",
        "    def step(self, action):\n",
        "        action = action.squeeze().to(torch.float32) # Ensure action is 1D and float32\n",
        "\n",
        "        # Enhanced action processing with momentum\n",
        "        momentum_factor = 0.9\n",
        "        if self.prev_action is not None:\n",
        "            action = momentum_factor * self.prev_action + (1 - momentum_factor) * action\n",
        "        self.prev_action = action.detach().clone() # Store detached copy for next step\n",
        "\n",
        "        scaled_action = action * self.difficulty * 0.8\n",
        "\n",
        "        # Update entanglement and chaoticity\n",
        "        new_entanglement_level = self.tensor.get_entanglement_level() + scaled_action[0]\n",
        "        new_chaoticity_index = self.tensor.get_chaoticity_index() + scaled_action[1]\n",
        "        self.tensor.set_entanglement_level(new_entanglement_level)\n",
        "        self.tensor.set_chaoticity_index(new_chaoticity_index)\n",
        "\n",
        "        current_tensor_data = self.tensor.get_current_tensor_data()\n",
        "        current_entanglement_level = self.tensor.get_entanglement_level()\n",
        "        current_chaoticity_index = self.tensor.get_chaoticity_index()\n",
        "\n",
        "        # Evolve the tensor\n",
        "        evolved_tensor_data = self.tensor(\n",
        "            current_tensor_data,\n",
        "            current_entanglement_level,\n",
        "            current_chaoticity_index\n",
        "        )\n",
        "        self.tensor.set_current_tensor_data(evolved_tensor_data)\n",
        "\n",
        "        # Project to consciousness spectrum\n",
        "        projected_spectrum = self.kernel(\n",
        "            self.tensor.get_current_tensor_data(),\n",
        "            self.tensor.get_entanglement_level()\n",
        "        )\n",
        "\n",
        "        # Enhanced Reward Calculation\n",
        "        spectral_similarity_loss = F.mse_loss(projected_spectrum, self.target)\n",
        "        spectral_similarity = torch.exp(-spectral_similarity_loss * 3.0) # Higher means better similarity\n",
        "\n",
        "        optimal_entanglement = 0.618\n",
        "        entanglement_deviation = (current_entanglement_level - optimal_entanglement) ** 2\n",
        "\n",
        "        metrics = self.tensor.metrics()\n",
        "        # Ensure metrics values are tensors for gradient tracking if needed for reward.backward()\n",
        "        complexity_reward = torch.tensor(metrics['complexity'], device=self.device, dtype=torch.float32) # Explicit dtype\n",
        "        phase_coherence_reward = torch.tensor(metrics['phase_coherence'], device=self.device, dtype=torch.float32) # Explicit dtype\n",
        "\n",
        "        stability_penalty = torch.tensor(0.0, device=self.device, dtype=torch.float32) # Explicit dtype\n",
        "        if len(self.tensor.history) >= 2: # Need at least 2 for previous state\n",
        "            current_state_detached = evolved_tensor_data.detach().cpu().numpy() # This is already float32 from tensor\n",
        "            prev_state = self.tensor.history[-2] # This is already float32 numpy\n",
        "\n",
        "            # Convert to torch.tensor for MSE loss, ensures correct device and dtype\n",
        "            prev_state_tensor = torch.tensor(prev_state, device=self.device, dtype=torch.float32)\n",
        "            current_state_tensor = torch.tensor(current_state_detached, device=self.device, dtype=torch.float32)\n",
        "\n",
        "            if len(self.tensor.history) >= 3: # Need at least 3 for third-last state\n",
        "                third_last_state = self.tensor.history[-3] # Already numpy float32\n",
        "                third_last_state_tensor = torch.tensor(third_last_state, device=self.device, dtype=torch.float32)\n",
        "                stability_penalty = F.mse_loss(current_state_tensor, prev_state_tensor) * 0.5 + F.mse_loss(current_state_tensor, third_last_state_tensor) * 0.5\n",
        "            else:\n",
        "                stability_penalty = F.mse_loss(current_state_tensor, prev_state_tensor)\n",
        "\n",
        "        # Entropic cost and balance\n",
        "        t_normalized_for_grad = F.softmax(evolved_tensor_data.flatten(), dim=0) + 1e-8\n",
        "        entropic_cost = -(t_normalized_for_grad * torch.log(t_normalized_for_grad)).sum()\n",
        "        optimal_entropy = np.log(len(t_normalized_for_grad)) * 0.75 # Optimal entropy for a certain level of randomness\n",
        "        entropy_balance = torch.abs(entropic_cost - optimal_entropy).to(torch.float32) # Ensure float32\n",
        "\n",
        "        # Fractal reward calculation: closer to 1.8 is better\n",
        "        fractal_reward = torch.exp(-torch.abs(\n",
        "            torch.tensor(metrics['fractal_dimension'], device=self.device, dtype=torch.float32) - 1.8 # Explicit dtype\n",
        "        ))\n",
        "\n",
        "        # Combine all rewards and penalties\n",
        "        reward = (\n",
        "            3.0 * spectral_similarity +\n",
        "            0.5 * complexity_reward +\n",
        "            0.5 * phase_coherence_reward +\n",
        "            0.5 * fractal_reward -\n",
        "            0.1 * entanglement_deviation -\n",
        "            0.3 * stability_penalty -\n",
        "            0.2 * entropy_balance\n",
        "        ).to(torch.float32) # Ensure final reward is float32\n",
        "\n",
        "        # Track spectral similarity for curriculum learning within the episode\n",
        "        self.current_episode_similarities.append(spectral_similarity.item())\n",
        "\n",
        "        # Update state history for agent observation\n",
        "        current_tensor_flat = self.tensor.get_current_tensor_data().detach().flatten().cpu().numpy().astype(np.float32) # Explicitly cast to float32 NumPy array\n",
        "        state_dim = current_tensor_flat.shape[0]\n",
        "\n",
        "        if self.state_history_buffer is None:\n",
        "            self.state_history_buffer = [np.zeros(state_dim, dtype=np.float32)] * 10 # Initialize with float32 zeros\n",
        "\n",
        "        self.state_history_buffer.pop(0)\n",
        "        self.state_history_buffer.append(current_tensor_flat)\n",
        "        state_sequence_for_agent = np.array(self.state_history_buffer, dtype=np.float32) # Ensure output array is float32\n",
        "\n",
        "        # Increment time and check termination\n",
        "        self.time += 1\n",
        "        done = self.time >= self.max_time\n",
        "\n",
        "        # Update curriculum at episode end\n",
        "        if done:\n",
        "            episode_performance = np.mean(self.current_episode_similarities)\n",
        "            self.performance_history.append(episode_performance)\n",
        "            self.current_episode_similarities = [] # Reset for next episode\n",
        "            self.episode_count += 1 # Increment episode count AFTER performance is recorded\n",
        "            self.update_curriculum()\n",
        "\n",
        "        # Prepare info dictionary\n",
        "        info = {\n",
        "            \"spectral_similarity\": spectral_similarity.item(),\n",
        "            \"complexity\": complexity_reward.item(),\n",
        "            \"phase_coherence\": phase_coherence_reward.item(),\n",
        "            \"fractal_reward\": fractal_reward.item(),\n",
        "            \"entanglement_deviation\": entanglement_deviation.item(),\n",
        "            \"stability_penalty\": stability_penalty.item(),\n",
        "            \"entropy_balance\": entropy_balance.item(),\n",
        "            \"metrics\": metrics, # Include all raw metrics for detailed logging\n",
        "            \"difficulty\": self.difficulty # Include current difficulty\n",
        "        }\n",
        "\n",
        "        return torch.FloatTensor(state_sequence_for_agent).unsqueeze(0).to(self.device), reward, done, info # Return reward as tensor\n",
        "\n",
        "    def reset(self):\n",
        "        self.tensor.reset()\n",
        "        self.time = 0\n",
        "        self.prev_action = None\n",
        "        self.current_episode_similarities = [] # Reset episode-specific metrics\n",
        "\n",
        "        # Initialize state_history_buffer with zeros for the new episode\n",
        "        state_dim = np.prod(self.tensor.initial_size)\n",
        "        self.state_history_buffer = [np.zeros(state_dim, dtype=np.float32)] * 10\n",
        "\n",
        "        # Return initial state sequence (all zeros initially as nothing has evolved yet)\n",
        "        return torch.FloatTensor(np.array(self.state_history_buffer, dtype=np.float32)).unsqueeze(0).to(self.device) # Explicit dtype\n",
        "\n",
        "# ===== TRAINING LOOP =====\n",
        "\n",
        "def train_rdt_system(num_episodes=500, visualize_every=50):\n",
        "    env = RDTEnvironment(device)\n",
        "    state_dim = np.prod(env.tensor.initial_size)\n",
        "    action_dim = 2\n",
        "    agent = RCSAgent(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=action_dim\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(agent.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_episodes)\n",
        "\n",
        "    history = {\n",
        "        'rewards': [], 'entanglement': [], 'chaoticity': [], 'entropy': [],\n",
        "        'spectral_similarity': [], 'entanglement_deviation_abs': [],\n",
        "        'entropic_cost': [], 'complexity': [], 'fractal_dimension': [],\n",
        "        'phase_coherence': [], 'learning_rate': [], 'loss': [],\n",
        "        'value_predictions': [], 'uncertainties': [], 'difficulty': []\n",
        "    }\n",
        "\n",
        "    best_reward = float('-inf')\n",
        "    best_model_path = os.path.join(results_dir, \"rdt_agent_best.pth\")\n",
        "\n",
        "    print(f\"Starting RDT System Training on {device}\")\n",
        "\n",
        "    for episode in tqdm(range(num_episodes), desc=\"Training RDT System\"):\n",
        "        state = env.reset() # This returns the initial state sequence\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        episode_metrics = {\n",
        "            'reward_sum': 0, 'loss_sum': 0, 'spectral_similarity_sum': 0,\n",
        "            'entanglement_deviation_sum': 0, 'entropic_cost_sum': 0,\n",
        "            'complexity_sum': 0, 'phase_coherence_sum': 0, 'fractal_sum': 0,\n",
        "            'value_sum': 0, 'uncertainty_sum': 0, 'step_count': 0\n",
        "        }\n",
        "\n",
        "        # Inner loop for steps within an episode\n",
        "        for step_idx in range(env.max_time): # Iterate up to max_time, `done` can break early\n",
        "            # Get action from agent\n",
        "            action, value_prediction, state_embedding, uncertainty = agent(state)\n",
        "\n",
        "            # Environment step\n",
        "            next_state, reward_tensor, done, info = env.step(action) # reward_tensor is already a tensor\n",
        "\n",
        "            # Accumulate metrics for current episode\n",
        "            episode_metrics['reward_sum'] += reward_tensor.item() # Use .item() for sum, keep tensor for loss\n",
        "            episode_metrics['spectral_similarity_sum'] += info['spectral_similarity']\n",
        "            episode_metrics['entanglement_deviation_sum'] += abs(info['entanglement_deviation'])\n",
        "            episode_metrics['entropic_cost_sum'] += info['entropy_balance'] # Use entropy_balance as cost\n",
        "            episode_metrics['complexity_sum'] += info['complexity']\n",
        "            episode_metrics['phase_coherence_sum'] += info['phase_coherence']\n",
        "            episode_metrics['fractal_sum'] += info['metrics']['fractal_dimension']\n",
        "            episode_metrics['value_sum'] += value_prediction.item()\n",
        "            episode_metrics['uncertainty_sum'] += uncertainty.item()\n",
        "            episode_metrics['step_count'] += 1\n",
        "\n",
        "            # Calculate losses\n",
        "            actor_loss = -reward_tensor # reward_tensor is already a float32 tensor\n",
        "            critic_loss = F.mse_loss(value_prediction, reward_tensor.detach().unsqueeze(0)) # Unsqueeze for batch dim\n",
        "\n",
        "            # Action entropy: encourages exploration\n",
        "            action_dist = torch.distributions.Normal(action.mean(), action.std().clamp(min=1e-8))\n",
        "            action_entropy = action_dist.entropy().mean()\n",
        "            exploration_bonus = 0.005 * action_entropy # Small bonus for entropy\n",
        "\n",
        "            total_loss = actor_loss + 0.5 * critic_loss - exploration_bonus\n",
        "            episode_metrics['loss_sum'] += total_loss.item()\n",
        "\n",
        "            # Optimization step\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=1.0) # Clip gradients to prevent exploding gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update state for next step\n",
        "            state = next_state # next_state is already the sequence\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Step scheduler after each episode\n",
        "        scheduler.step()\n",
        "\n",
        "        # Record episode averages into history\n",
        "        avg_steps = max(1, episode_metrics['step_count'])\n",
        "        history['rewards'].append(episode_metrics['reward_sum'])\n",
        "        history['entanglement'].append(env.tensor.get_entanglement_level().detach().item())\n",
        "        history['chaoticity'].append(env.tensor.get_chaoticity_index().detach().item())\n",
        "        history['entropy'].append(env.tensor.metrics()['entropy']) # Get current entropy from tensor\n",
        "        history['spectral_similarity'].append(episode_metrics['spectral_similarity_sum'] / avg_steps)\n",
        "        history['entanglement_deviation_abs'].append(episode_metrics['entanglement_deviation_sum'] / avg_steps)\n",
        "        history['entropic_cost'].append(episode_metrics['entropic_cost_sum'] / avg_steps)\n",
        "        history['complexity'].append(episode_metrics['complexity_sum'] / avg_steps)\n",
        "        history['fractal_dimension'].append(episode_metrics['fractal_sum'] / avg_steps)\n",
        "        history['phase_coherence'].append(episode_metrics['phase_coherence_sum'] / avg_steps)\n",
        "        history['learning_rate'].append(scheduler.get_last_lr()[0])\n",
        "        history['loss'].append(episode_metrics['loss_sum'] / avg_steps)\n",
        "        history['value_predictions'].append(episode_metrics['value_sum'] / avg_steps)\n",
        "        history['uncertainties'].append(episode_metrics['uncertainty_sum'] / avg_steps)\n",
        "        history['difficulty'].append(env.difficulty)\n",
        "\n",
        "        # Save best model\n",
        "        current_reward = episode_metrics['reward_sum']\n",
        "        if current_reward > best_reward:\n",
        "            best_reward = current_reward\n",
        "            torch.save(agent.state_dict(), best_model_path)\n",
        "\n",
        "        # Enhanced logging\n",
        "        if (episode + 1) % 25 == 0 or episode == num_episodes - 1:\n",
        "            tqdm.write(f\"Ep {episode+1}/{num_episodes} | R: {history['rewards'][-1]:.3f} | \"\n",
        "                      f\"E: {history['entanglement'][-1]:.3f} | C: {history['chaoticity'][-1]:.3f} | \"\n",
        "                      f\"Sim: {history['spectral_similarity'][-1]:.3f} | Comp: {history['complexity'][-1]:.3f} | \"\n",
        "                      f\"Uncert: {history['uncertainties'][-1]:.3f} | LR: {history['learning_rate'][-1]:.6f} | \"\n",
        "                      f\"Diff: {history['difficulty'][-1]:.2f}\")\n",
        "\n",
        "            # Visualize progress periodically\n",
        "            visualize_progress(env, agent, episode, history['rewards'], history, final_save=False)\n",
        "\n",
        "    # Final visualization and save\n",
        "    visualize_progress(env, agent, num_episodes, history['rewards'], history, final_save=True)\n",
        "\n",
        "    # Save final model and history\n",
        "    torch.save(agent.state_dict(), os.path.join(results_dir, \"rdt_agent_final.pth\"))\n",
        "    # Save DynamicTensor state if needed (e.g., its parameters)\n",
        "    torch.save(env.tensor.state_dict(), os.path.join(results_dir, \"dynamic_tensor_final.pth\"))\n",
        "\n",
        "    history_path = os.path.join(results_dir, \"training_history.json\")\n",
        "    with open(history_path, \"w\") as f:\n",
        "        # Convert all numpy arrays/tensors in history to lists for JSON serialization\n",
        "        serializable_history = {k: [item if isinstance(item, (int, float, str)) else item.tolist() if isinstance(item, np.ndarray) else item for item in v] for k, v in history.items()}\n",
        "        json.dump(serializable_history, f, indent=4)\n",
        "\n",
        "    print(f\"Training complete. Results saved to {results_dir}\")\n",
        "\n",
        "    # Return the trained agent and environment\n",
        "    return agent, env\n",
        "\n",
        "# ===== VISUALIZATION UTILITIES =====\n",
        "\n",
        "def visualize_progress(env, agent, episode, episode_rewards, history, final_save=False):\n",
        "    plt.figure(figsize=(18, 12))\n",
        "\n",
        "    # Ensure history keys are lists and not dicts of dicts for plotting\n",
        "    # Plot reward progression\n",
        "    plt.subplot(2, 3, 1)\n",
        "    if episode_rewards:\n",
        "        plt.plot(episode_rewards)\n",
        "        plt.title(f\"Episode Rewards (Current: {episode_rewards[-1]:.2f})\")\n",
        "    else:\n",
        "        plt.title(\"Episode Rewards (No data)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.grid(True, alpha=0.6)\n",
        "\n",
        "    # Plot key metrics (last N steps for recent trend or all if few episodes)\n",
        "    plt.subplot(2, 3, 2)\n",
        "    metrics_to_plot = ['spectral_similarity', 'complexity', 'phase_coherence']\n",
        "    plot_steps = min(len(history['spectral_similarity']), 100) # Plot last 100 steps\n",
        "\n",
        "    if plot_steps > 0:\n",
        "        for metric in metrics_to_plot:\n",
        "            plt.plot(history[metric][-plot_steps:], label=metric)\n",
        "        plt.title(f\"Recent Metric Values (Last {plot_steps} Steps)\")\n",
        "        plt.xlabel(\"Relative Step in Window\")\n",
        "        plt.ylabel(\"Value\")\n",
        "        plt.legend()\n",
        "    else:\n",
        "        plt.title(\"Recent Metric Values (No data)\")\n",
        "    plt.grid(True, alpha=0.6)\n",
        "\n",
        "    # Plot tensor evolution (a slice or flattened version of the last state)\n",
        "    plt.subplot(2, 3, 3)\n",
        "    if env.tensor.history:\n",
        "        tensor_data_to_plot = env.tensor.history[-1]\n",
        "        if len(tensor_data_to_plot.shape) == 3:\n",
        "            plt.imshow(tensor_data_to_plot.mean(axis=0), cmap='viridis') # Mean across first dim if 3D\n",
        "            plt.title(\"Current Tensor State (Mean Slice)\")\n",
        "            plt.colorbar(label=\"Value\")\n",
        "        elif len(tensor_data_to_plot.shape) == 2:\n",
        "            plt.imshow(tensor_data_to_plot, cmap='viridis') # If 2D\n",
        "            plt.title(\"Current Tensor State (2D)\")\n",
        "            plt.colorbar(label=\"Value\")\n",
        "        else: # Flattened 1D representation\n",
        "            plt.plot(tensor_data_to_plot.flatten())\n",
        "            plt.title(\"Current Tensor State (Flattened)\")\n",
        "            plt.ylabel(\"Value\")\n",
        "    else:\n",
        "        plt.title(\"Current Tensor State (No data)\")\n",
        "    plt.xlabel(\"Index\")\n",
        "    plt.grid(True, alpha=0.6)\n",
        "\n",
        "    # Plot fractal dimension evolution\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plot_steps_fd = min(len(history['fractal_dimension']), 100)\n",
        "    if plot_steps_fd > 0:\n",
        "        plt.plot(history['fractal_dimension'][-plot_steps_fd:])\n",
        "        plt.axhline(y=1.8, color='r', linestyle='--', label='Target FD (1.8)') # Updated target\n",
        "        plt.title(\"Fractal Dimension Evolution\")\n",
        "        plt.xlabel(\"Relative Episode in Window\")\n",
        "        plt.ylabel(\"Dimension\")\n",
        "        plt.legend()\n",
        "    else:\n",
        "        plt.title(\"Fractal Dimension Evolution (No data)\")\n",
        "    plt.grid(True, alpha=0.6)\n",
        "\n",
        "    # Plot phase coherence\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plot_steps_pc = min(len(history['phase_coherence']), 100)\n",
        "    if plot_steps_pc > 0:\n",
        "        plt.plot(history['phase_coherence'][-plot_steps_pc:])\n",
        "        plt.title(\"Phase Coherence\")\n",
        "        plt.xlabel(\"Relative Episode in Window\")\n",
        "        plt.ylabel(\"Coherence (0-1)\")\n",
        "    else:\n",
        "        plt.title(\"Phase Coherence (No data)\")\n",
        "    plt.grid(True, alpha=0.6)\n",
        "\n",
        "    # Plot entanglement vs chaoticity\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plot_steps_ec = min(len(history['entanglement']), 100)\n",
        "    if plot_steps_ec > 0:\n",
        "        entanglements = history['entanglement'][-plot_steps_ec:]\n",
        "        chaoticities = history['chaoticity'][-plot_steps_ec:]\n",
        "        plt.scatter(entanglements, chaoticities, c=range(len(entanglements)), cmap='viridis', s=20)\n",
        "        plt.title(\"Entanglement vs Chaoticity\")\n",
        "        plt.xlabel(\"Entanglement\")\n",
        "        plt.ylabel(\"Chaoticity\")\n",
        "        plt.colorbar(label=\"Time Step (Relative)\")\n",
        "    else:\n",
        "        plt.title(\"Entanglement vs Chaoticity (No data)\")\n",
        "    plt.grid(True, alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename_suffix = \"final_training_progress\" if final_save else f\"training_progress_episode_{episode+1}\"\n",
        "    plot_path = os.path.join(results_dir, f\"{filename_suffix}.png\")\n",
        "    plt.savefig(plot_path, dpi=300 if final_save else 200, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# ===== MAIN EXECUTION =====\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Starting RDT System Training on {device}\")\n",
        "    trained_agent, trained_env = train_rdt_system(num_episodes=500)\n",
        "\n",
        "    print(f\"Training complete. Results saved to {results_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvS4N8iJM5ar",
        "outputId": "df434122-5b39-49e4-c309-4f55b2acf817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting RDT System Training on cuda\n",
            "Starting RDT System Training on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:   5%|         | 24/500 [00:14<04:39,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 25/500 | R: 41.897 | E: 0.010 | C: 0.800 | Sim: 0.659 | Comp: 0.148 | Uncert: 0.890 | LR: 0.000099 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  10%|         | 49/500 [00:29<04:00,  1.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 50/500 | R: 41.955 | E: 0.010 | C: 0.800 | Sim: 0.655 | Comp: 0.176 | Uncert: 0.930 | LR: 0.000098 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  15%|        | 74/500 [00:43<03:37,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 75/500 | R: 42.256 | E: 0.010 | C: 0.800 | Sim: 0.658 | Comp: 0.146 | Uncert: 0.972 | LR: 0.000095 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  20%|        | 99/500 [00:59<03:28,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 100/500 | R: 41.632 | E: 0.010 | C: 0.800 | Sim: 0.647 | Comp: 0.149 | Uncert: 0.979 | LR: 0.000090 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  25%|       | 124/500 [01:14<03:50,  1.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 125/500 | R: 41.988 | E: 0.010 | C: 0.800 | Sim: 0.646 | Comp: 0.177 | Uncert: 0.987 | LR: 0.000085 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  30%|       | 149/500 [01:29<03:08,  1.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 150/500 | R: 40.130 | E: 0.010 | C: 0.800 | Sim: 0.622 | Comp: 0.165 | Uncert: 0.990 | LR: 0.000079 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  35%|      | 174/500 [01:44<02:47,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 175/500 | R: 43.339 | E: 0.010 | C: 0.800 | Sim: 0.665 | Comp: 0.167 | Uncert: 0.995 | LR: 0.000073 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  40%|      | 199/500 [01:59<02:32,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 200/500 | R: 44.643 | E: 0.010 | C: 0.800 | Sim: 0.663 | Comp: 0.192 | Uncert: 0.996 | LR: 0.000065 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  45%|     | 224/500 [02:15<02:55,  1.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 225/500 | R: 40.247 | E: 0.010 | C: 0.800 | Sim: 0.630 | Comp: 0.178 | Uncert: 0.996 | LR: 0.000058 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  50%|     | 249/500 [02:31<02:13,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 250/500 | R: 43.811 | E: 0.010 | C: 0.800 | Sim: 0.661 | Comp: 0.190 | Uncert: 0.996 | LR: 0.000050 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  55%|    | 274/500 [02:46<01:56,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 275/500 | R: 43.427 | E: 0.010 | C: 0.800 | Sim: 0.658 | Comp: 0.191 | Uncert: 0.998 | LR: 0.000042 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  60%|    | 299/500 [03:02<01:54,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 300/500 | R: 43.367 | E: 0.010 | C: 0.800 | Sim: 0.655 | Comp: 0.175 | Uncert: 0.996 | LR: 0.000035 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  65%|   | 324/500 [03:18<01:39,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 325/500 | R: 39.888 | E: 0.010 | C: 0.800 | Sim: 0.603 | Comp: 0.170 | Uncert: 0.997 | LR: 0.000027 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  70%|   | 349/500 [03:33<01:19,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 350/500 | R: 43.042 | E: 0.010 | C: 0.800 | Sim: 0.639 | Comp: 0.149 | Uncert: 0.998 | LR: 0.000021 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  75%|  | 374/500 [03:48<01:08,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 375/500 | R: 43.182 | E: 0.010 | C: 0.800 | Sim: 0.671 | Comp: 0.190 | Uncert: 0.999 | LR: 0.000015 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  80%|  | 399/500 [04:04<01:04,  1.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 400/500 | R: 44.056 | E: 0.010 | C: 0.800 | Sim: 0.669 | Comp: 0.213 | Uncert: 0.999 | LR: 0.000010 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  85%| | 424/500 [04:20<00:41,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 425/500 | R: 44.379 | E: 0.010 | C: 0.800 | Sim: 0.666 | Comp: 0.196 | Uncert: 0.998 | LR: 0.000005 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  90%| | 449/500 [04:35<00:26,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 450/500 | R: 42.738 | E: 0.010 | C: 0.800 | Sim: 0.659 | Comp: 0.176 | Uncert: 0.998 | LR: 0.000002 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System:  95%|| 474/500 [04:50<00:13,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 475/500 | R: 41.940 | E: 0.010 | C: 0.800 | Sim: 0.660 | Comp: 0.155 | Uncert: 0.998 | LR: 0.000001 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System: 100%|| 499/500 [05:06<00:00,  1.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 500/500 | R: 43.069 | E: 0.010 | C: 0.800 | Sim: 0.657 | Comp: 0.242 | Uncert: 0.999 | LR: 0.000000 | Diff: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training RDT System: 100%|| 500/500 [05:08<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete. Results saved to rdt_results_20250720_234442\n",
            "Training complete. Results saved to rdt_results_20250720_234442\n"
          ]
        }
      ]
    }
  ]
}